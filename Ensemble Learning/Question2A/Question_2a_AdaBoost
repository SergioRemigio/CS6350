'''
Author: Sergio Remigio
Date: 3/12/2021

This file contains the implementation of the Ada Boost algorithm that
use a stum decision tree as its weak learner for the Bank examples.
'''
import csv
import math
import copy
maximumTreeDepth = 1


allAttributesWithValues = {
        "age": [38, 38.0000000001],
        "job": ["admin.", "unknown", "unemployed", "management", "housemaid", "entrepreneur", "student",
                "blue-collar", "self-employed", "retired", "technician", "services"],
        "marital": ["married", "divorced", "single"],
        "education": ["unknown", "secondary", "primary", "tertiary"],
        "default": ["yes", "no"],
        "balance": [453, 453.500000001],
        "housing": ["yes", "no"],
        "loan": ["yes", "no"],
        "contact": ["unknown", "telephone", "cellular"],
        "day": [16, 16.0000000001],
        "month": ["jan", "feb", "mar", "apr", "may", "jun",
                  "jul", "aug", "sep", "oct", "nov", "dec"],
        "duration": [180, 180.0000000001],
        "campaign": [2, 2.0000000001],
        "pdays": [-1, -0.9999999999],
        "previous": [0, 0.0000000001],
        "poutcome": ["unknown", "other", "failure", "success"]
    }



def isInteger(x):
    try:
        int(x)
        return True
    except ValueError:
        return False



'''
Children list is a list of all of the
'''
class Tree:
    def __init__(self):
        self.children = list()
        self.attribute = None
        self.branch = None
        self.nextTree = None
'''
Given a set, returns a list containing
the probabilities of every label in order:
1. "yes"
2. "no"
Weights of the examples are taken into consideration when giving result
'''
def findProbabilitiesOfLabels(examplesSet):

    labelsCount = {
    "yes": 0,
    "no": 0
    }

    if isinstance(examplesSet[0], list):
        totalCount = len(examplesSet)
        for example in examplesSet:
            nextLabel = example[16]
            labelsCount[nextLabel] += (1 * example[17])
    # There is only one example in the set
    else:
        totalCount = 1
        nextLabel = examplesSet[16]
        labelsCount[nextLabel] += 1
    probOfLabels = list()
    # Return a list of the probabilities
    for label in labelsCount.keys():
        labelProbability = (labelsCount[label] / totalCount)
        probOfLabels.append(labelProbability)
    # Normalizes the probabilities so that the sum of the probabilities = 1
    z = 0
    for prob in probOfLabels:
        z += prob
    for i in range(len(probOfLabels)):
        probOfLabels[i] = probOfLabels[i] / z
    return probOfLabels

'''
Entropy{p1, p2, ..., pk} = -(\sum_{i=1}(pilog_4(pi)))
where p = the probability of kth label
(i.e., probability of "unacc", "acc".. etc.)
Weights of the examples are taken into consideration when giving result
'''
def findEntropy(examplesSet):
    pk = findProbabilitiesOfLabels(examplesSet)

    result = 0
    for p in pk:
        if p != 0:
            result = result + (p * math.log(p,2.0))

    result = -result

    return result

'''
Returns a Dictionary of size n attributes containing:
A dictionary of values of a single attribute with a list of
all example subsets for that attribute
'''
def getValueSubsetsForEveryAttribute(examplesSet, attributes):
    dict = {}

    #Loop through all of the examples for all attributes
    #and fill dictionary in with subsets of all value examples
    for i in range(16):
        if attributes[i] != -1:
            #Dictionary containing a subset(list of examples) for every value in attributes
            valueSubSets = {}

            #Create empty dictionary that will hold an attribute(key) and subset examples(value)
            for value in allAttributesWithValues[attributes[i]]:
                valueSubSets.update({value: list()})



            # Fill the dictionary
            # The example is a numeric type
            for example in examplesSet:
                if isInteger(example[i]):
                    currNum = int(example[i])

                    upperBound = allAttributesWithValues[attributes[i]][0]
                    lowerBound = allAttributesWithValues[attributes[i]][1]
                    if currNum <= upperBound:
                        valueSubSets[upperBound].append(example)
                    if currNum > lowerBound:
                        valueSubSets[lowerBound].append(example)
                else:
                    valueSubSets[example[i]].append(example)

            #update dict to return later
            dict.update({attributes[i]: valueSubSets})

    return dict



'''
Returns the best attribute to split the example set on
using Entropy
'''
def findBestAttributeToSplitOnEntroypy(examplesSet, attributes):
    currentEntropy = findEntropy(examplesSet)

    listOfGainsToCalculate = getValueSubsetsForEveryAttribute(examplesSet, attributes)
    largestGain = 0
    bestAttribute = None
    # Return a random attribute if all are the same
    for x in range(16):
        if attributes[x] != -1:
            bestAttribute = attributes[x]

    for attribute in listOfGainsToCalculate.keys():
        gain = 0
        for value in listOfGainsToCalculate[attribute].keys():
            valueSubSet = listOfGainsToCalculate[attribute][value]
            numerator = len(valueSubSet)
            denominator = 0
            for attributeValue in listOfGainsToCalculate[attribute].keys():
                denominator = denominator + len(listOfGainsToCalculate[attribute][attributeValue])
            weight = numerator/denominator
            if not valueSubSet:
                continue
            gain = gain + (weight * findEntropy(valueSubSet))
        # Return a random attribute if all attributes have 0 gain
        gain = currentEntropy - gain
        if gain > largestGain:
            largestGain = gain
            bestAttribute = attribute
    return bestAttribute


def importExamples(file):
    examplesSet = list()

    with open(file, 'r') as f:
        for line in f:
            terms = line.strip().split(',')
            examplesSet.append(terms)
    return examplesSet


def predictLabel(set):
    allLabels = list(('yes', 'no'))
    prediction = 'no'

    for label in allLabels:
        allSame = True
        for example in set:
            if example[16] != label:
                allSame = False
                break
        if allSame == True:
            prediction = label
            break
    return prediction


def checkAllSameLabel(examplesSet, label):
    for example in examplesSet:
        if example[16] != label:
            return False
    return True


'''
Counts the number of -1 in the
attributes list as that correlates to the
the current tree depth
'''
def getTreeDepth(attributes):
    depth = 0
    for attribute in attributes:
        if attribute == -1:
            depth = depth + 1

    return depth

'''
Decision tree algorithm
s: the set of Examples
Label: the target attribute(the prediction)
Attributs: the set of measerude attributes
'''
def ID3Algorithm(examplesSet, label, attributes):
    #Checks whether maximum depth has been reached
    treeDepth = getTreeDepth(attributes)
    if treeDepth == maximumTreeDepth:
        #Return a leaf node with the most common label
        leaf = findMostCommonLabelInS(examplesSet)
        return leaf

    #if all examples have the same label
    if checkAllSameLabel(examplesSet, label) == True:
        #If attributes empty
        if not attributes:
            #Return a leaf node with the most common label
            leaf = findMostCommonLabelInS(examplesSet)
            return leaf

        #else return a leaf node with the label
        return label

    #Create a root node for tree
    root = Tree()

    A = findBestAttributeToSplitOnEntroypy(examplesSet, attributes)
    AttributeColumn = list(allAttributesWithValues.keys()).index(A)
    root.attribute = A



    #For each possible value v of that A can take:
    for value in allAttributesWithValues[A]:
        child = Tree()
        # Add a new tree branch corresponding to A=v
        child.branch = value
        root.children.append(child)
        Sv = list()
        if isinstance(value, str):
            # Let Sv be the subset of examples in S with A = v
            for example in examplesSet:
                if example[AttributeColumn] == value:
                    Sv.append(example)
        # This means the value is a numeric type
        else:
            # If the value tag is an int, then we check for example values less or equal to the value tag.
            if isinstance(value, int):
                for example in examplesSet:
                    if int(example[AttributeColumn]) <= value:
                        Sv.append(example)
            # If the value tag is an float, then we check for example values greater than the value tag.
            if isinstance(value, float):
                for example in examplesSet:
                    if int(example[AttributeColumn]) > value:
                        Sv.append(example)


        #If Sv is empty:
        if not Sv:
            #Add leaf node with the most common value of Label in S
            leaf = findMostCommonLabelInS(examplesSet)
            child.nextTree = leaf
        else:
            #Below this branch add the next subtree
            labelPrediction = predictLabel(Sv)
            #Remove attribute A from all attributes
            newAttributes = copy.deepcopy(attributes)
            newAttributes[attributes.index(A)] = -1
            child.nextTree = ID3Algorithm(Sv, labelPrediction, newAttributes)
    #Return Root node
    return root

'''
Returns the index of the attribute
'''
def getIndex(attribute, attributes):
    for i in range(16):
        if attribute == attributes[i]:
            return i
'''
Returns the a predicted label based on the decision tree given
'''
def decisionTreePrediction(example, root, attributes):
    ##If reached a leaf node, return the label
    if isinstance(root, str):
        return root

    # Attribute that was split on
    attribute = root.attribute
    # Column of the attribute that was split on
    i = getIndex(attribute, attributes)
    testValue = example[i]
    # Check every child to see what path the example must take in the decision tree
    for child in root.children:
        if isinstance(child.branch, int):
            if int(testValue) <= child.branch:
                return decisionTreePrediction(example, child.nextTree, attributes)
        elif isinstance(child.branch, float):
            if int(testValue) > child.branch:
                return decisionTreePrediction(example, child.nextTree, attributes)
        else:
            if child.branch == testValue:
                return decisionTreePrediction(example, child.nextTree, attributes)


'''---------------------------AdaBoost methods below-------------------------------------------------'''

'''
Calculates the total predicted correct and returns the prediction rate
'''
def findAveragePredictionSuccess(hFinal, examplesSet, attributes):
    totalCorrect = 0
    for example in examplesSet:
        actualResult = example[16]
        prediction = adaBoostPrediction(example, hFinal, attributes)
        if prediction == actualResult:
            totalCorrect = totalCorrect + 1

    return totalCorrect / len(examplesSet)


'''
Returns an adaBoostPrediction for the example given
'''
def adaBoostPrediction(example, hFinal, attributes):
    voteCount = {
        "yes": 0,
        "no": 0
    }
    # Add up all the votes while taking into account their vote weight
    for stump in hFinal:
        vote = decisionTreePrediction(example, stump.ht, attributes)
        voteCount[vote] += (1 * stump.alpha_t)
    # Count the votes and return the majority vote
    if voteCount["yes"] > voteCount["no"]:
        return "yes"
    return "no"


'''
Returns the most common label in the given examples.
AdaBoost: Weight is considered when returning the most common label
'''
def findMostCommonLabelInS(examplesSet):
    labelsCount = {
    "yes": 0,
    "no": 0
    }

    for example in examplesSet:
        nextLabel = example[16]
        # The weight is considered here by multiplying by the weight instead of treating everything as one
        labelsCount[nextLabel] += (1 * example[17])

    mostCommonLabel = "yes"
    for nextLabel in labelsCount.keys():
        if labelsCount[mostCommonLabel] < labelsCount[nextLabel]:
            mostCommonLabel = nextLabel

    return mostCommonLabel


'''
Sets the initial weight of every example to be 1/m 
where m is the size of the example set.
The weight will also be appended to the example 
(i.e., example[17] will hold the weight for that example)
'''
def giveInitialWeight(examplesSet):
    initialWeight = 1/len(examplesSet)
    for example in examplesSet:
        example.append(initialWeight)


'''
The true error of ht is simply the sum of 
all the weight of the examples where yi != h(xi)
'''
def findTrueErrorOfht(ht, examplesSet, attributes):
    trueError = 0
    for example in examplesSet:
        actualResult = example[16]
        prediction = decisionTreePrediction(example, ht, attributes)
        if actualResult != prediction:
            # Add the weight the example where yi != h(xi)
            trueError += example[17]
    return trueError


'''
D_t+1(i) = D_t(i)/Zt * e(alpha_t) 
where Z is a normalization constant to ensure all the D_t + 1 weights add up to 1
'''
def updateExampleWeight(alpha_t, ht, attributes, examplesSet):
    zt = 0
    for example in examplesSet:
        actualResult = example[16]
        prediction = decisionTreePrediction(example, ht, attributes)
        newWeight = 0
        if actualResult != prediction:
            # Increase weight if miss classified example
            newWeight = example[17] * math.pow(math.e, alpha_t)
        else:
            # Decrease weight if correctly classified example
            newWeight = example[17] * math.pow(math.e, -alpha_t)
        zt += newWeight
        example[17] = newWeight
    # Normalize the weights so that the sum of all the weights add up to 1
    for example in examplesSet:
        example[17] = example[17] / zt


'''
Represents a stump of a decision tree. 
This class is only used to hold the decision tree stump 
and its alpha value.
The alpha value is used as the weight this decision stump has when voting. 
'''
class Stump:
    def __init__(self, ht, alpha_t):
        self.ht = ht
        self.alpha_t = alpha_t


'''
Ada boost algorithm.
Returns: A list of HFinal that contains T decision stumps with their alpha value used in voting 
'''
def adaBoost(T, examplesSet, attributes):
    hFinal = list()
    for t in range(T):
        if t == 45:
            print()
        # classifier ht whose weighted classification error is better than chance
        ht = ID3Algorithm(examplesSet, 'yes', attributes)
        trueError = findTrueErrorOfht(ht, examplesSet, attributes)
        # Compute its vote
        alpha_t = None
        if trueError > .5:
            alpha_t = 0
        else:
            alpha_t = (1/2) * math.log(((1-trueError)/trueError))
        # Update values of the weights for the training examples
        updateExampleWeight(alpha_t, ht, attributes, examplesSet)
        # Adds the new stump to the hFinal where it will be used at the end to predict example labels
        newStump = Stump(ht, alpha_t)
        hFinal.append(newStump)
    return hFinal


def resetToInitialWeight(examplesSet):
    m = len(examplesSet)
    for example in examplesSet:
        example[17] = 1/m


def main():
    attributes = ['age', 'job', 'marital', 'education', 'default', 'balance'
                      , 'housing', 'loan', 'contact', 'day', 'month', 'duration', 'campaign'
                      , 'pdays', 'previous', 'poutcome']
    examplesSet = importExamples('train.csv')
    giveInitialWeight(examplesSet)
    testExamples = importExamples('test.csv')
    giveInitialWeight(testExamples)

    print("\t\t Training \t Testing:")
    # Prints from t values 1-10
    for T in range(1, 11):
        hFinal = adaBoost(T, examplesSet, attributes)
        resetToInitialWeight(examplesSet)
        print("T = " + str(T) + ':\t\t' +
              str(findAveragePredictionSuccess(hFinal, examplesSet, attributes)) + "\t\t" +
              str(findAveragePredictionSuccess(hFinal, testExamples, attributes)))
    for T in range(1, 11):
        hFinal = adaBoost(T * 10, examplesSet, attributes)
        resetToInitialWeight(examplesSet)
        print("T = " + str(T * 10) + ':\t\t' +
              str(findAveragePredictionSuccess(hFinal, examplesSet, attributes)) + "\t\t" +
              str(findAveragePredictionSuccess(hFinal, testExamples, attributes)))
    # Prints from T values 100-500 in 100 increments
    for T in range(1, 6):
        hFinal = adaBoost(T * 100, examplesSet, attributes)
        resetToInitialWeight(examplesSet)
        print("T = " + str(T * 100) + ':\t\t' +
              str(findAveragePredictionSuccess(hFinal, examplesSet, attributes)) + "\t\t" +
              str(findAveragePredictionSuccess(hFinal, testExamples, attributes)))
main()

